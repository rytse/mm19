{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.gaussian_process.kernels import RBF, ConstantKernel as C, Matern, WhiteKernel\n",
    "from sklearn import linear_model\n",
    "from sklearn.metrics import log_loss\n",
    "\n",
    "from scipy.stats import zscore, norm\n",
    "\n",
    "import itertools\n",
    "\n",
    "np.random.seed(1)\n",
    "\n",
    "# Load data (training and validation)\n",
    "train_raw_teamstats = pd.read_excel('cts.xlsx', sheet_name='2017-2018')\n",
    "train_raw_games = pd.read_csv('clean_2017-2018_combo.csv')\n",
    "val_raw_teamstats = pd.read_excel('cts.xlsx', sheet_name='2016-2017')\n",
    "val_raw_games = pd.read_csv('clean_2016-2017_combo.csv')\n",
    "\n",
    "# Clean all the bloody whitespace\n",
    "train_raw_teamstats = train_raw_teamstats.rename(columns=lambda x: x.strip())\n",
    "train_raw_games = train_raw_games.rename(columns=lambda x: x.strip())\n",
    "val_raw_teamstats = val_raw_teamstats.rename(columns=lambda x: x.strip())\n",
    "val_raw_games = val_raw_games.rename(columns=lambda x: x.strip())\n",
    "\n",
    "# Get the list of all stats\n",
    "VARS = list(train_raw_teamstats)\n",
    "VARS.remove('Team Name')\n",
    "\n",
    "def train_on_tk(v2u, rbest, obest, wbest):\n",
    "    winner_vars = []\n",
    "    loser_vars = []\n",
    "    for v in v2u:\n",
    "        if v != 'Team Name':\n",
    "            winner_vars.append(v + ' Winner')\n",
    "            loser_vars.append(v + ' Loser')\n",
    "    \n",
    "\n",
    "    # Load each training game into a np array that will be the\n",
    "    # training data set\n",
    "    Xs = []    # input vars\n",
    "    ys = []    # output var\n",
    "    for idx, row in train_raw_games.iterrows():\n",
    "        w_v = np.array(row[winner_vars])\n",
    "        l_v = np.array(row[loser_vars])\n",
    "        Xs.append(np.array([w_v, l_v]).flatten())\n",
    "        ys.append(np.array([row['Winner Points'] - row['Loser points']]))\n",
    "        Xs.append(np.array([l_v, w_v]).flatten())\n",
    "        ys.append(np.array([row['Loser points'] - row['Winner Points']]))\n",
    "\n",
    "    # Convert lists to np array\n",
    "    X = np.array(Xs)\n",
    "    y = np.array(ys)\n",
    "\n",
    "    # Load each training game into a np array that will be the\n",
    "    # validation data set\n",
    "    Xvs = []\n",
    "    yvs = []\n",
    "    for idx, row in val_raw_games.iterrows():\n",
    "        winner = row['Winner']\n",
    "        loser = row['Loser']\n",
    "\n",
    "        try:\n",
    "            w_v = np.array(row[winner_vars])\n",
    "            l_v = np.array(row[loser_vars])\n",
    "            Xvs.append(np.array([w_v, l_v]).flatten())\n",
    "            yvs.append(np.array([row['Winner Points'] - row['Loser points']]))\n",
    "            Xvs.append(np.array([l_v, w_v]).flatten())\n",
    "            yvs.append(np.array([row['Loser points'] - row['Winner Points']]))\n",
    "        except:\n",
    "            print(winner)\n",
    "            print(loser)\n",
    "\n",
    "    Xv = np.array(Xvs)\n",
    "    yv = np.array(yvs)\n",
    "\n",
    "    # Train a Gaussian Process Regression model\n",
    "\n",
    "    kernel = obest * RBF(rbest) + WhiteKernel(wbest)\n",
    "    gp = GaussianProcessRegressor(kernel,\n",
    "                                n_restarts_optimizer=10,\n",
    "                                normalize_y=True)\n",
    "    gp.fit(X, y)\n",
    "    rep = gp.score(Xv, yv)\n",
    "\n",
    "    # Train the linear model on the same data\n",
    "    lm = linear_model.LinearRegression(fit_intercept=False, normalize=True)\n",
    "    lm.fit(X, y)\n",
    "    lm.score(Xv, yv)\n",
    "\n",
    "    gp_y_pred = []\n",
    "    lm_y_pred = []\n",
    "    gp_sd = []\n",
    "    tys = []\n",
    "    for idx, row in val_raw_games.iterrows():\n",
    "        winner = row['Winner']\n",
    "        loser = row['Loser']\n",
    "\n",
    "        try:\n",
    "            w_v = np.array(row[winner_vars])\n",
    "            l_v = np.array(row[loser_vars])\n",
    "            Xvs.append(np.array([w_v, l_v]).flatten())\n",
    "            yvs.append(np.array([row['Winner Points'] - row['Loser points']]))\n",
    "\n",
    "            gp_y_pred_, gp_sd_ = gp.predict(np.array([w_v, l_v]).flatten().reshape(1, -1), return_std=True)\n",
    "            gp_y_pred.append(gp_y_pred_)\n",
    "            gp_sd.append(gp_sd_)\n",
    "            lm_y_pred.append(lm.predict(np.array([w_v, l_v]).flatten().reshape(1, -1)))\n",
    "            tys.append(row['Winner Points'] - row['Loser points'])\n",
    "        except:\n",
    "            print(winner)\n",
    "            print(loser)\n",
    "\n",
    "    gp_y_pred=np.array(gp_y_pred).flatten()\n",
    "    lm_y_pred=np.array(lm_y_pred).flatten()\n",
    "    gp_sd = np.array(gp_sd).flatten()\n",
    "    tys = np.array(tys).flatten()\n",
    "\n",
    "    correct = 0\n",
    "    for i in range(len(tys)):\n",
    "        if tys[i] >= 0:\n",
    "            if lm_y_pred[i] >= 0:\n",
    "                correct += 1\n",
    "        else:\n",
    "            if lm_y_pred[i] < 0:\n",
    "                correct += 1\n",
    "\n",
    "    correct = 0\n",
    "    for i in range(len(tys)):\n",
    "        if tys[i] >= 0:\n",
    "            if gp_y_pred[i] >= 0:\n",
    "                correct += 1\n",
    "        else:\n",
    "            if gp_y_pred[i] < 0:\n",
    "                correct += 1\n",
    "                \n",
    "    y_pred_probs = 1 - norm.cdf(-gp_y_pred / gp_sd)\n",
    "    ll = log_loss(np.array(tys > 0, dtype=np.int), np.array([1 - y_pred_probs, y_pred_probs]).T, labels=[0, 1])\n",
    "\n",
    "    return ll, gp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "RBF_LENGTH_SCALE = 54.5618245614035\n",
    "RBF_KERNEL_SCALE = 18.538748538011692\n",
    "NOISE_LENGTH_SCALE = 84.34343436464646"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0%\n",
      "10%\n",
      "20%\n",
      "30%\n",
      "40%\n",
      "50%\n",
      "60%\n",
      "70%\n",
      "80%\n",
      "90%\n",
      "100%\n",
      "110%\n",
      "120%\n",
      "130%\n",
      "140%\n",
      "150%\n",
      "160%\n",
      "170%\n",
      "180%\n",
      "190%\n",
      "rbest: 67.48436195752538\n",
      "obest: 14.147992305324713\n",
      "wbest: 42.17171718232323\n"
     ]
    }
   ],
   "source": [
    "max_rbest = None\n",
    "max_obest = None\n",
    "max_wbest = None\n",
    "max_ll = 1\n",
    "\n",
    "to_keep = ['Adj Off Efficiency', 'Adj Def Efficiency', 'Turnovers per game', 'Wins Last 10 Games',\n",
    "           'Points Allowed Per Game']\n",
    "c = 0\n",
    "for rbest in np.linspace(RBF_LENGTH_SCALE * 0.5, RBF_LENGTH_SCALE * 1.5, 20):\n",
    "    print(f'{c}%')\n",
    "    c += 10\n",
    "    for obest in np.linspace(RBF_KERNEL_SCALE * 0.5, RBF_KERNEL_SCALE * 1.5, 20):\n",
    "        for wbest in np.linspace(NOISE_LENGTH_SCALE * 0.5, NOISE_LENGTH_SCALE * 1.5, 20):\n",
    "            ll = train_on_tk(['Team Name'] + list(to_keep), rbest, obest, wbest)\n",
    "#             print(ll)\n",
    "            if ll < max_ll:\n",
    "                max_ll = ll\n",
    "                max_obest = obest\n",
    "                max_rbest = rbest\n",
    "                max_wbest = wbest\n",
    "                \n",
    "print('rbest: ' + str(max_rbest))\n",
    "print('obest: ' + str(max_obest))\n",
    "print('wbest: ' + str(max_wbest))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.48077324679830874"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_ll"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "RBF_LENGTH_SCALE = max_rbest\n",
    "RBF_KERNEL_SCALE = max_obest\n",
    "NOISE_LENGTH_SCALE = max_wbest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0%\n",
      "5%\n",
      "10%\n",
      "15%\n",
      "20%\n",
      "25%\n",
      "30%\n",
      "35%\n",
      "40%\n",
      "45%\n",
      "rbest: 93.72828049656303\n",
      "obest: 11.789993587770596\n",
      "wbest: 25.771604944753086\n",
      "log loss: 0.4807585789840988\n"
     ]
    }
   ],
   "source": [
    "c = 0\n",
    "for rbest in np.linspace(RBF_LENGTH_SCALE * 0.5, RBF_LENGTH_SCALE * 1.5, 10):\n",
    "    print(f'{c}%')\n",
    "    c += 5\n",
    "    for obest in np.linspace(RBF_KERNEL_SCALE * 0.5, RBF_KERNEL_SCALE * 1.5, 10):\n",
    "        for wbest in np.linspace(NOISE_LENGTH_SCALE * 0.5, NOISE_LENGTH_SCALE * 1.5, 10):\n",
    "            ll = train_on_tk(['Team Name'] + list(to_keep), rbest, obest, wbest)\n",
    "#             print(ll)\n",
    "            if ll < max_ll:\n",
    "                max_ll = ll\n",
    "                max_obest = obest\n",
    "                max_rbest = rbest\n",
    "                max_wbest = wbest\n",
    "                \n",
    "print('rbest: ' + str(max_rbest))\n",
    "print('obest: ' + str(max_obest))\n",
    "print('wbest: ' + str(max_wbest))\n",
    "print('log loss: ' + str(max_ll))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0%\n",
      "5%\n",
      "10%\n",
      "15%\n",
      "20%\n",
      "25%\n",
      "30%\n",
      "35%\n",
      "40%\n",
      "45%\n",
      "rbest: 93.72828049656303\n",
      "obest: 11.789993587770596\n",
      "wbest: 25.771604944753086\n",
      "log loss: 0.4807585789840988\n"
     ]
    }
   ],
   "source": [
    "RBF_LENGTH_SCALE = max_rbest\n",
    "RBF_KERNEL_SCALE = max_obest\n",
    "NOISE_LENGTH_SCALE = max_wbest\n",
    "\n",
    "c = 0\n",
    "for rbest in np.linspace(RBF_LENGTH_SCALE * 0.5, RBF_LENGTH_SCALE * 1.5, 10):\n",
    "    print(f'{c}%')\n",
    "    c += 5\n",
    "    for obest in np.linspace(RBF_KERNEL_SCALE * 0.5, RBF_KERNEL_SCALE * 1.5, 10):\n",
    "        for wbest in np.linspace(NOISE_LENGTH_SCALE * 0.5, NOISE_LENGTH_SCALE * 1.5, 10):\n",
    "            ll = train_on_tk(['Team Name'] + list(to_keep), rbest, obest, wbest)\n",
    "#             print(ll)\n",
    "            if ll < max_ll:\n",
    "                max_ll = ll\n",
    "                max_obest = obest\n",
    "                max_rbest = rbest\n",
    "                max_wbest = wbest\n",
    "                \n",
    "print('rbest: ' + str(max_rbest))\n",
    "print('obest: ' + str(max_obest))\n",
    "print('wbest: ' + str(max_wbest))\n",
    "print('log loss: ' + str(max_ll))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "ll, gp = train_on_tk(['Team Name'] + list(to_keep), rbest, obest, wbest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.07444234],\n",
       "       [-0.07444234],\n",
       "       [ 0.05485087],\n",
       "       [-0.05485087],\n",
       "       [ 0.08598861],\n",
       "       [-0.08598861],\n",
       "       [ 0.04432642],\n",
       "       [-0.04432642],\n",
       "       [ 0.06906277],\n",
       "       [-0.06906277],\n",
       "       [ 0.11156787],\n",
       "       [-0.11156787],\n",
       "       [-0.03002762],\n",
       "       [ 0.03002762],\n",
       "       [ 0.04761487],\n",
       "       [-0.04761487],\n",
       "       [ 0.04052962],\n",
       "       [-0.04052962],\n",
       "       [-0.02330248],\n",
       "       [ 0.02330248],\n",
       "       [ 0.1341284 ],\n",
       "       [-0.1341284 ],\n",
       "       [ 0.15327887],\n",
       "       [-0.15327887],\n",
       "       [ 0.01886961],\n",
       "       [-0.01886961],\n",
       "       [ 0.04088163],\n",
       "       [-0.04088163],\n",
       "       [ 0.16077533],\n",
       "       [-0.16077533],\n",
       "       [-0.01847716],\n",
       "       [ 0.01847716],\n",
       "       [ 0.06999619],\n",
       "       [-0.06999619],\n",
       "       [ 0.2024802 ],\n",
       "       [-0.2024802 ],\n",
       "       [ 0.04153054],\n",
       "       [-0.04153054],\n",
       "       [ 0.22190537],\n",
       "       [-0.22190537],\n",
       "       [-0.03639917],\n",
       "       [ 0.03639917],\n",
       "       [ 0.03888323],\n",
       "       [-0.03888323],\n",
       "       [ 0.09856274],\n",
       "       [-0.09856274],\n",
       "       [ 0.11013406],\n",
       "       [-0.11013406],\n",
       "       [ 0.13144939],\n",
       "       [-0.13144939],\n",
       "       [ 0.09314204],\n",
       "       [-0.09314204],\n",
       "       [ 0.03238157],\n",
       "       [-0.03238157],\n",
       "       [ 0.04003197],\n",
       "       [-0.04003197],\n",
       "       [ 0.02496702],\n",
       "       [-0.02496702],\n",
       "       [-0.02169883],\n",
       "       [ 0.02169883],\n",
       "       [-0.02410775],\n",
       "       [ 0.02410775],\n",
       "       [ 0.07344931],\n",
       "       [-0.07344931],\n",
       "       [-0.01168873],\n",
       "       [ 0.01168873],\n",
       "       [ 0.09503366],\n",
       "       [-0.09503366],\n",
       "       [ 0.06777015],\n",
       "       [-0.06777015],\n",
       "       [ 0.0413551 ],\n",
       "       [-0.0413551 ],\n",
       "       [ 0.0955542 ],\n",
       "       [-0.0955542 ],\n",
       "       [ 0.0643539 ],\n",
       "       [-0.0643539 ],\n",
       "       [ 0.07370782],\n",
       "       [-0.07370782],\n",
       "       [ 0.07029082],\n",
       "       [-0.07029082],\n",
       "       [-0.05933773],\n",
       "       [ 0.05933773],\n",
       "       [-0.07760544],\n",
       "       [ 0.07760544],\n",
       "       [ 0.07095239],\n",
       "       [-0.07095239],\n",
       "       [ 0.05818384],\n",
       "       [-0.05818384],\n",
       "       [ 0.13131607],\n",
       "       [-0.13131607],\n",
       "       [ 0.01165444],\n",
       "       [-0.01165444],\n",
       "       [ 0.14440002],\n",
       "       [-0.14440002],\n",
       "       [ 0.00169977],\n",
       "       [-0.00169977],\n",
       "       [ 0.123639  ],\n",
       "       [-0.123639  ],\n",
       "       [-0.04207914],\n",
       "       [ 0.04207914],\n",
       "       [ 0.07035818],\n",
       "       [-0.07035818],\n",
       "       [ 0.02821314],\n",
       "       [-0.02821314],\n",
       "       [-0.00233886],\n",
       "       [ 0.00233886],\n",
       "       [-0.02063301],\n",
       "       [ 0.02063301],\n",
       "       [ 0.06013835],\n",
       "       [-0.06013835],\n",
       "       [ 0.07240829],\n",
       "       [-0.07240829],\n",
       "       [-0.03288354],\n",
       "       [ 0.03288354],\n",
       "       [ 0.00635861],\n",
       "       [-0.00635861],\n",
       "       [-0.02902933],\n",
       "       [ 0.02902933],\n",
       "       [ 0.22207903],\n",
       "       [-0.22207903],\n",
       "       [ 0.04109808],\n",
       "       [-0.04109808],\n",
       "       [ 0.01401168],\n",
       "       [-0.01401168],\n",
       "       [ 0.01051455],\n",
       "       [-0.01051455]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gp.alpha_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on GaussianProcessRegressor in module sklearn.gaussian_process.gpr object:\n",
      "\n",
      "class GaussianProcessRegressor(sklearn.base.BaseEstimator, sklearn.base.RegressorMixin)\n",
      " |  Gaussian process regression (GPR).\n",
      " |  \n",
      " |  The implementation is based on Algorithm 2.1 of Gaussian Processes\n",
      " |  for Machine Learning (GPML) by Rasmussen and Williams.\n",
      " |  \n",
      " |  In addition to standard scikit-learn estimator API,\n",
      " |  GaussianProcessRegressor:\n",
      " |  \n",
      " |     * allows prediction without prior fitting (based on the GP prior)\n",
      " |     * provides an additional method sample_y(X), which evaluates samples\n",
      " |       drawn from the GPR (prior or posterior) at given inputs\n",
      " |     * exposes a method log_marginal_likelihood(theta), which can be used\n",
      " |       externally for other ways of selecting hyperparameters, e.g., via\n",
      " |       Markov chain Monte Carlo.\n",
      " |  \n",
      " |  Read more in the :ref:`User Guide <gaussian_process>`.\n",
      " |  \n",
      " |  .. versionadded:: 0.18\n",
      " |  \n",
      " |  Parameters\n",
      " |  ----------\n",
      " |  kernel : kernel object\n",
      " |      The kernel specifying the covariance function of the GP. If None is\n",
      " |      passed, the kernel \"1.0 * RBF(1.0)\" is used as default. Note that\n",
      " |      the kernel's hyperparameters are optimized during fitting.\n",
      " |  \n",
      " |  alpha : float or array-like, optional (default: 1e-10)\n",
      " |      Value added to the diagonal of the kernel matrix during fitting.\n",
      " |      Larger values correspond to increased noise level in the observations.\n",
      " |      This can also prevent a potential numerical issue during fitting, by\n",
      " |      ensuring that the calculated values form a positive definite matrix.\n",
      " |      If an array is passed, it must have the same number of entries as the\n",
      " |      data used for fitting and is used as datapoint-dependent noise level.\n",
      " |      Note that this is equivalent to adding a WhiteKernel with c=alpha.\n",
      " |      Allowing to specify the noise level directly as a parameter is mainly\n",
      " |      for convenience and for consistency with Ridge.\n",
      " |  \n",
      " |  optimizer : string or callable, optional (default: \"fmin_l_bfgs_b\")\n",
      " |      Can either be one of the internally supported optimizers for optimizing\n",
      " |      the kernel's parameters, specified by a string, or an externally\n",
      " |      defined optimizer passed as a callable. If a callable is passed, it\n",
      " |      must have the signature::\n",
      " |  \n",
      " |          def optimizer(obj_func, initial_theta, bounds):\n",
      " |              # * 'obj_func' is the objective function to be minimized, which\n",
      " |              #   takes the hyperparameters theta as parameter and an\n",
      " |              #   optional flag eval_gradient, which determines if the\n",
      " |              #   gradient is returned additionally to the function value\n",
      " |              # * 'initial_theta': the initial value for theta, which can be\n",
      " |              #   used by local optimizers\n",
      " |              # * 'bounds': the bounds on the values of theta\n",
      " |              ....\n",
      " |              # Returned are the best found hyperparameters theta and\n",
      " |              # the corresponding value of the target function.\n",
      " |              return theta_opt, func_min\n",
      " |  \n",
      " |      Per default, the 'fmin_l_bfgs_b' algorithm from scipy.optimize\n",
      " |      is used. If None is passed, the kernel's parameters are kept fixed.\n",
      " |      Available internal optimizers are::\n",
      " |  \n",
      " |          'fmin_l_bfgs_b'\n",
      " |  \n",
      " |  n_restarts_optimizer : int, optional (default: 0)\n",
      " |      The number of restarts of the optimizer for finding the kernel's\n",
      " |      parameters which maximize the log-marginal likelihood. The first run\n",
      " |      of the optimizer is performed from the kernel's initial parameters,\n",
      " |      the remaining ones (if any) from thetas sampled log-uniform randomly\n",
      " |      from the space of allowed theta-values. If greater than 0, all bounds\n",
      " |      must be finite. Note that n_restarts_optimizer == 0 implies that one\n",
      " |      run is performed.\n",
      " |  \n",
      " |  normalize_y : boolean, optional (default: False)\n",
      " |      Whether the target values y are normalized, i.e., the mean of the\n",
      " |      observed target values become zero. This parameter should be set to\n",
      " |      True if the target values' mean is expected to differ considerable from\n",
      " |      zero. When enabled, the normalization effectively modifies the GP's\n",
      " |      prior based on the data, which contradicts the likelihood principle;\n",
      " |      normalization is thus disabled per default.\n",
      " |  \n",
      " |  copy_X_train : bool, optional (default: True)\n",
      " |      If True, a persistent copy of the training data is stored in the\n",
      " |      object. Otherwise, just a reference to the training data is stored,\n",
      " |      which might cause predictions to change if the data is modified\n",
      " |      externally.\n",
      " |  \n",
      " |  random_state : int, RandomState instance or None, optional (default: None)\n",
      " |      The generator used to initialize the centers. If int, random_state is\n",
      " |      the seed used by the random number generator; If RandomState instance,\n",
      " |      random_state is the random number generator; If None, the random number\n",
      " |      generator is the RandomState instance used by `np.random`.\n",
      " |  \n",
      " |  Attributes\n",
      " |  ----------\n",
      " |  X_train_ : array-like, shape = (n_samples, n_features)\n",
      " |      Feature values in training data (also required for prediction)\n",
      " |  \n",
      " |  y_train_ : array-like, shape = (n_samples, [n_output_dims])\n",
      " |      Target values in training data (also required for prediction)\n",
      " |  \n",
      " |  kernel_ : kernel object\n",
      " |      The kernel used for prediction. The structure of the kernel is the\n",
      " |      same as the one passed as parameter but with optimized hyperparameters\n",
      " |  \n",
      " |  L_ : array-like, shape = (n_samples, n_samples)\n",
      " |      Lower-triangular Cholesky decomposition of the kernel in ``X_train_``\n",
      " |  \n",
      " |  alpha_ : array-like, shape = (n_samples,)\n",
      " |      Dual coefficients of training data points in kernel space\n",
      " |  \n",
      " |  log_marginal_likelihood_value_ : float\n",
      " |      The log-marginal-likelihood of ``self.kernel_.theta``\n",
      " |  \n",
      " |  Examples\n",
      " |  --------\n",
      " |  >>> from sklearn.datasets import make_friedman2\n",
      " |  >>> from sklearn.gaussian_process import GaussianProcessRegressor\n",
      " |  >>> from sklearn.gaussian_process.kernels import DotProduct, WhiteKernel\n",
      " |  >>> X, y = make_friedman2(n_samples=500, noise=0, random_state=0)\n",
      " |  >>> kernel = DotProduct() + WhiteKernel()\n",
      " |  >>> gpr = GaussianProcessRegressor(kernel=kernel,\n",
      " |  ...         random_state=0).fit(X, y)\n",
      " |  >>> gpr.score(X, y) # doctest: +ELLIPSIS\n",
      " |  0.3680...\n",
      " |  >>> gpr.predict(X[:2,:], return_std=True) # doctest: +ELLIPSIS\n",
      " |  (array([653.0..., 592.1...]), array([316.6..., 316.6...]))\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      GaussianProcessRegressor\n",
      " |      sklearn.base.BaseEstimator\n",
      " |      sklearn.base.RegressorMixin\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, kernel=None, alpha=1e-10, optimizer='fmin_l_bfgs_b', n_restarts_optimizer=0, normalize_y=False, copy_X_train=True, random_state=None)\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |  \n",
      " |  fit(self, X, y)\n",
      " |      Fit Gaussian process regression model.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like, shape = (n_samples, n_features)\n",
      " |          Training data\n",
      " |      \n",
      " |      y : array-like, shape = (n_samples, [n_output_dims])\n",
      " |          Target values\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self : returns an instance of self.\n",
      " |  \n",
      " |  log_marginal_likelihood(self, theta=None, eval_gradient=False)\n",
      " |      Returns log-marginal likelihood of theta for training data.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      theta : array-like, shape = (n_kernel_params,) or None\n",
      " |          Kernel hyperparameters for which the log-marginal likelihood is\n",
      " |          evaluated. If None, the precomputed log_marginal_likelihood\n",
      " |          of ``self.kernel_.theta`` is returned.\n",
      " |      \n",
      " |      eval_gradient : bool, default: False\n",
      " |          If True, the gradient of the log-marginal likelihood with respect\n",
      " |          to the kernel hyperparameters at position theta is returned\n",
      " |          additionally. If True, theta must not be None.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      log_likelihood : float\n",
      " |          Log-marginal likelihood of theta for training data.\n",
      " |      \n",
      " |      log_likelihood_gradient : array, shape = (n_kernel_params,), optional\n",
      " |          Gradient of the log-marginal likelihood with respect to the kernel\n",
      " |          hyperparameters at position theta.\n",
      " |          Only returned when eval_gradient is True.\n",
      " |  \n",
      " |  predict(self, X, return_std=False, return_cov=False)\n",
      " |      Predict using the Gaussian process regression model\n",
      " |      \n",
      " |      We can also predict based on an unfitted model by using the GP prior.\n",
      " |      In addition to the mean of the predictive distribution, also its\n",
      " |      standard deviation (return_std=True) or covariance (return_cov=True).\n",
      " |      Note that at most one of the two can be requested.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like, shape = (n_samples, n_features)\n",
      " |          Query points where the GP is evaluated\n",
      " |      \n",
      " |      return_std : bool, default: False\n",
      " |          If True, the standard-deviation of the predictive distribution at\n",
      " |          the query points is returned along with the mean.\n",
      " |      \n",
      " |      return_cov : bool, default: False\n",
      " |          If True, the covariance of the joint predictive distribution at\n",
      " |          the query points is returned along with the mean\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      y_mean : array, shape = (n_samples, [n_output_dims])\n",
      " |          Mean of predictive distribution a query points\n",
      " |      \n",
      " |      y_std : array, shape = (n_samples,), optional\n",
      " |          Standard deviation of predictive distribution at query points.\n",
      " |          Only returned when return_std is True.\n",
      " |      \n",
      " |      y_cov : array, shape = (n_samples, n_samples), optional\n",
      " |          Covariance of joint predictive distribution a query points.\n",
      " |          Only returned when return_cov is True.\n",
      " |  \n",
      " |  sample_y(self, X, n_samples=1, random_state=0)\n",
      " |      Draw samples from Gaussian process and evaluate at X.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like, shape = (n_samples_X, n_features)\n",
      " |          Query points where the GP samples are evaluated\n",
      " |      \n",
      " |      n_samples : int, default: 1\n",
      " |          The number of samples drawn from the Gaussian process\n",
      " |      \n",
      " |      random_state : int, RandomState instance or None, optional (default=0)\n",
      " |          If int, random_state is the seed used by the random number\n",
      " |          generator; If RandomState instance, random_state is the\n",
      " |          random number generator; If None, the random number\n",
      " |          generator is the RandomState instance used by `np.random`.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      y_samples : array, shape = (n_samples_X, [n_output_dims], n_samples)\n",
      " |          Values of n_samples samples drawn from Gaussian process and\n",
      " |          evaluated at query points.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors defined here:\n",
      " |  \n",
      " |  rng\n",
      " |      DEPRECATED: Attribute rng was deprecated in version 0.19 and will be removed in 0.21.\n",
      " |  \n",
      " |  y_train_mean\n",
      " |      DEPRECATED: Attribute y_train_mean was deprecated in version 0.19 and will be removed in 0.21.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.base.BaseEstimator:\n",
      " |  \n",
      " |  __getstate__(self)\n",
      " |  \n",
      " |  __repr__(self)\n",
      " |      Return repr(self).\n",
      " |  \n",
      " |  __setstate__(self, state)\n",
      " |  \n",
      " |  get_params(self, deep=True)\n",
      " |      Get parameters for this estimator.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      deep : boolean, optional\n",
      " |          If True, will return the parameters for this estimator and\n",
      " |          contained subobjects that are estimators.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      params : mapping of string to any\n",
      " |          Parameter names mapped to their values.\n",
      " |  \n",
      " |  set_params(self, **params)\n",
      " |      Set the parameters of this estimator.\n",
      " |      \n",
      " |      The method works on simple estimators as well as on nested objects\n",
      " |      (such as pipelines). The latter have parameters of the form\n",
      " |      ``<component>__<parameter>`` so that it's possible to update each\n",
      " |      component of a nested object.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from sklearn.base.BaseEstimator:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.base.RegressorMixin:\n",
      " |  \n",
      " |  score(self, X, y, sample_weight=None)\n",
      " |      Returns the coefficient of determination R^2 of the prediction.\n",
      " |      \n",
      " |      The coefficient R^2 is defined as (1 - u/v), where u is the residual\n",
      " |      sum of squares ((y_true - y_pred) ** 2).sum() and v is the total\n",
      " |      sum of squares ((y_true - y_true.mean()) ** 2).sum().\n",
      " |      The best possible score is 1.0 and it can be negative (because the\n",
      " |      model can be arbitrarily worse). A constant model that always\n",
      " |      predicts the expected value of y, disregarding the input features,\n",
      " |      would get a R^2 score of 0.0.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like, shape = (n_samples, n_features)\n",
      " |          Test samples. For some estimators this may be a\n",
      " |          precomputed kernel matrix instead, shape = (n_samples,\n",
      " |          n_samples_fitted], where n_samples_fitted is the number of\n",
      " |          samples used in the fitting for the estimator.\n",
      " |      \n",
      " |      y : array-like, shape = (n_samples) or (n_samples, n_outputs)\n",
      " |          True values for X.\n",
      " |      \n",
      " |      sample_weight : array-like, shape = [n_samples], optional\n",
      " |          Sample weights.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      score : float\n",
      " |          R^2 of self.predict(X) wrt. y.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(gp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'alpha': 1e-10,\n",
       " 'copy_X_train': True,\n",
       " 'kernel__k1': 4.21**2 * RBF(length_scale=141),\n",
       " 'kernel__k2': WhiteKernel(noise_level=38.7),\n",
       " 'kernel__k1__k1': 4.21**2,\n",
       " 'kernel__k1__k2': RBF(length_scale=141),\n",
       " 'kernel__k1__k1__constant_value': 17.684990381655894,\n",
       " 'kernel__k1__k1__constant_value_bounds': (1e-05, 100000.0),\n",
       " 'kernel__k1__k2__length_scale': 140.59242074484456,\n",
       " 'kernel__k1__k2__length_scale_bounds': (1e-05, 100000.0),\n",
       " 'kernel__k2__noise_level': 38.65740741712963,\n",
       " 'kernel__k2__noise_level_bounds': (1e-05, 100000.0),\n",
       " 'kernel': 4.21**2 * RBF(length_scale=141) + WhiteKernel(noise_level=38.7),\n",
       " 'n_restarts_optimizer': 10,\n",
       " 'normalize_y': True,\n",
       " 'optimizer': 'fmin_l_bfgs_b',\n",
       " 'random_state': None}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gp.get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-496.5289267805103"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gp.log_marginal_likelihood_value_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.51486295e+01  0.00000000e+00  0.00000000e+00 ...  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00]\n",
      " [ 1.65450543e+00  1.50580074e+01  0.00000000e+00 ...  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00]\n",
      " [ 2.95741244e+00  1.72602078e+00  1.47565422e+01 ...  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00]\n",
      " ...\n",
      " [ 2.25769697e+00  2.03023928e+00  3.46570156e+00 ...  1.14992670e+01\n",
      "   0.00000000e+00  0.00000000e+00]\n",
      " [ 1.07517603e+00  2.12249773e+00  2.11452952e+00 ... -1.09946050e-02\n",
      "   1.16716354e+01  0.00000000e+00]\n",
      " [ 2.22722928e+00  8.36928821e-01  2.57067003e+00 ... -4.35616771e-02\n",
      "   2.91029535e-01  1.16680064e+01]]\n"
     ]
    }
   ],
   "source": [
    "print(gp.L_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
